{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['UNKNOWN', 'NON-SMOKER', 'PAST SMOKER', 'CURRENT SMOKER']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "PATH = os.path.abspath(os.getcwd())\n",
    "datasetPath = PATH+\"/Case_Presentation/\"\n",
    "TraindatasetPath = datasetPath+\"train/\"\n",
    "TestdatasetPath = datasetPath+\"test/\"\n",
    "\n",
    "#Make Label file\n",
    "FileList = os.listdir(TraindatasetPath)\n",
    "ClassName = []\n",
    "\n",
    "for i in range(len(FileList)):\n",
    "    if FileList[i] != 'train.csv' and FileList[i] != 'classes.txt' and FileList[i] != '.ipynb_checkpoints':\n",
    "        tmpName = FileList[i].split(\"_\",1)[0]\n",
    "        try:\n",
    "            ClassName.index(tmpName)\n",
    "        except :\n",
    "            ClassName.append(tmpName)\n",
    "        \n",
    "print(ClassName)\n",
    "\n",
    "file = open(datasetPath+'classes.txt','w');\n",
    "for i in range(len(ClassName)):\n",
    "    file.write(str(ClassName[i])+'\\n');\n",
    "file.close();\n",
    "\n",
    "#Creat Training file\n",
    "CLASSNAME = []\n",
    "CLASSNAMES = []\n",
    "INFO = []\n",
    "FileList = os.listdir(TraindatasetPath)\n",
    "\n",
    "\n",
    "for i in range(len(FileList)):\n",
    "    filename = FileList[i]\n",
    "    tmpName = filename.split(\"_\",1)[0]\n",
    "    classN = ClassName.index(tmpName)\n",
    "    try:\n",
    "        classN = ClassName.index(tmpName)\n",
    "        CLASSNAMES.append(tmpName)\n",
    "        CLASSNAME.append(classN)\n",
    "    \n",
    "        file = open(TraindatasetPath+filename,'r')\n",
    "        info = file.read()\n",
    "        file.close()\n",
    "        INFO.append(info)\n",
    "        \n",
    "    except :\n",
    "        print(\"except {}\".format(tmpName))\n",
    "    \n",
    "\n",
    "df_train = pd.DataFrame({'label1':CLASSNAME,'label2':CLASSNAMES,'text':INFO})\n",
    "df_train.plot.hist()\n",
    "df_train.to_csv(datasetPath+\"train.csv\",index=False,sep=',',header=False)\n",
    "\n",
    "#Creat Testing file\n",
    "CLASSNAME = []\n",
    "CLASSNAMES = []\n",
    "INFO = []\n",
    "FileList = os.listdir(TestdatasetPath)\n",
    "\n",
    "\n",
    "for i in range(len(FileList)):\n",
    "    filename = FileList[i]    \n",
    "    tmpName = filename.split(\"_\",1)[0]\n",
    "    \n",
    "    CLASSNAME.append(0)\n",
    "    CLASSNAMES.append(ClassName[0])\n",
    "    \n",
    "    file = open(TestdatasetPath+filename,'r')\n",
    "    info = file.read()\n",
    "    file.close()\n",
    "    INFO.append(info)\n",
    "        \n",
    "import pandas as pd\n",
    "df_test = pd.DataFrame({'label1':CLASSNAME,'label2':CLASSNAMES,'text':INFO})\n",
    "df_test.to_csv(datasetPath+\"test.csv\",index=False,sep=',',header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import torch\n",
    "import io\n",
    "from torchtext.utils import download_from_url, extract_archive, unicode_csv_reader\n",
    "from torchtext.data.utils import ngrams_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.vocab import Vocab\n",
    "from tqdm import tqdm\n",
    "\n",
    "URLS = {\n",
    "    'Smoking_Status':\n",
    "        'https://drive.google.com/uc?export=download&id=0Bz8a_Dbh9QhbUDNpeUdjb0wxRms'\n",
    "}\n",
    "\n",
    "\n",
    "def _csv_iterator(data_path, ngrams, yield_cls=False):\n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "    with io.open(data_path, encoding=\"utf8\") as f:\n",
    "        reader = unicode_csv_reader(f)\n",
    "        for row in reader:\n",
    "            tokens = ' '.join(row[1:])\n",
    "            tokens = tokenizer(tokens)\n",
    "            if yield_cls:\n",
    "                #yield int(row[0]) - 1, ngrams_iterator(tokens, ngrams)\n",
    "                yield int(row[0]), ngrams_iterator(tokens, ngrams)\n",
    "            else:\n",
    "                yield ngrams_iterator(tokens, ngrams)\n",
    "\n",
    "\n",
    "def _create_data_from_iterator(vocab, iterator, include_unk):\n",
    "    data = []\n",
    "    labels = []\n",
    "    with tqdm(unit_scale=0, unit='lines') as t:\n",
    "        for cls, tokens in iterator:\n",
    "            if include_unk:\n",
    "                tokens = torch.tensor([vocab[token] for token in tokens])\n",
    "            else:\n",
    "                token_ids = list(filter(lambda x: x is not Vocab.UNK, [vocab[token]\n",
    "                                        for token in tokens]))\n",
    "                tokens = torch.tensor(token_ids)\n",
    "            if len(tokens) == 0:\n",
    "                logging.info('Row contains no tokens.')\n",
    "            data.append((cls, tokens))\n",
    "            labels.append(cls)\n",
    "            t.update(1)\n",
    "    return data, set(labels)\n",
    "\n",
    "\n",
    "class TextClassificationDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Defines an abstract text classification datasets.\n",
    "       Currently, we only support the following datasets:\n",
    "             - AG_NEWS\n",
    "             - SogouNews\n",
    "             - DBpedia\n",
    "             - YelpReviewPolarity\n",
    "             - YelpReviewFull\n",
    "             - YahooAnswers\n",
    "             - AmazonReviewPolarity\n",
    "             - AmazonReviewFull\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab, data, labels):\n",
    "        \"\"\"Initiate text-classification dataset.\n",
    "        Arguments:\n",
    "            vocab: Vocabulary object used for dataset.\n",
    "            data: a list of label/tokens tuple. tokens are a tensor after\n",
    "                numericalizing the string tokens. label is an integer.\n",
    "                [(label1, tokens1), (label2, tokens2), (label2, tokens3)]\n",
    "            label: a set of the labels.\n",
    "                {label1, label2}\n",
    "        Examples:\n",
    "            See the examples in examples/text_classification/\n",
    "        \"\"\"\n",
    "\n",
    "        super(TextClassificationDataset, self).__init__()\n",
    "        self._data = data\n",
    "        self._labels = labels\n",
    "        self._vocab = vocab\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self._data[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for x in self._data:\n",
    "            yield x\n",
    "\n",
    "    def get_labels(self):\n",
    "        return self._labels\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return self._vocab\n",
    "\n",
    "\n",
    "def _setup_datasets(dataset_name, root='.data', ngrams=1, vocab=None, include_unk=False):\n",
    "    #dataset_tar = download_from_url(URLS[dataset_name], root=root)\n",
    "    #extracted_files = extract_archive(dataset_tar)\n",
    "    rootPath =root\n",
    "    extracted_files = [rootPath+'train.csv',rootPath+'test.csv',rootPath+'classes.txt',rootPath+'readme.txt']   \n",
    "    for fname in extracted_files:\n",
    "        if fname.endswith('train.csv'):\n",
    "            train_csv_path = fname\n",
    "        #if fname.endswith('test.csv'):\n",
    "        if fname.endswith('train.csv'):    \n",
    "            test_csv_path = fname\n",
    "\n",
    "    if vocab is None:\n",
    "        print('Building Vocab based on {}'.format(train_csv_path))\n",
    "        vocab = build_vocab_from_iterator(_csv_iterator(train_csv_path, ngrams))\n",
    "    else:\n",
    "        if not isinstance(vocab, Vocab):\n",
    "            raise TypeError(\"Passed vocabulary is not of type Vocab\")\n",
    "    print('Vocab has {} entries'.format(len(vocab)))\n",
    "    \n",
    "    print('Creating training data')\n",
    "    train_data, train_labels = _create_data_from_iterator(\n",
    "        vocab, _csv_iterator(train_csv_path, ngrams, yield_cls=True), include_unk)\n",
    "    \n",
    "    print('Creating testing data')\n",
    "    test_data, test_labels = _create_data_from_iterator(\n",
    "        vocab, _csv_iterator(test_csv_path, ngrams, yield_cls=True), include_unk)\n",
    "    \n",
    "    \n",
    "    print(train_labels)\n",
    "    print(test_labels)\n",
    "    \n",
    "   \n",
    "    if len(train_labels ^ test_labels) > 0:\n",
    "        raise ValueError(\"Training and test labels don't match\")\n",
    "    return (TextClassificationDataset(vocab, train_data, train_labels),\n",
    "            TextClassificationDataset(vocab, test_data, test_labels))\n",
    "\n",
    "\n",
    "def Smoking_Status(*args, **kwargs):\n",
    "    \"\"\" Defines Smoking_Status datasets.\n",
    "        The labels includes:\n",
    "            - 0 : UNKNOWN\n",
    "            - 1 : NON-SMOKER\n",
    "            - 2 : PAST SMOKER\n",
    "            - 3 : CURRENT SMOKER\n",
    "    Create supervised learning dataset: Smoking_Status\n",
    "    Separately returns the training and test dataset\n",
    "    Arguments:\n",
    "        root: Directory where the datasets are saved. Default: \".data\"\n",
    "        ngrams: a contiguous sequence of n items from s string text.\n",
    "            Default: 1\n",
    "        vocab: Vocabulary used for dataset. If None, it will generate a new\n",
    "            vocabulary based on the train data set.\n",
    "        include_unk: include unknown token in the data (Default: False)\n",
    "    Examples:\n",
    "        >>> train_dataset, test_dataset = torchtext.datasets.Smoking_Status(ngrams=3)\n",
    "    \"\"\"\n",
    "\n",
    "    return _setup_datasets(*((\"Smoking_Status\",) + args), **kwargs)\n",
    "\n",
    "\n",
    "DATASETS = {\n",
    "    'Smoking_Status': Smoking_Status\n",
    "}\n",
    "\n",
    "\n",
    "LABELS = {\n",
    "    'Smoking_Status': {0: 'UNKNOWN',\n",
    "                1: 'NON-SMOKER',\n",
    "                2: 'PAST SMOKER',\n",
    "                3: 'CURRENT SMOKER'}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40lines [00:00, 1482.03lines/s]\n",
      "40lines [00:00, 1161.65lines/s]\n",
      "40lines [00:00, 1172.72lines/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Vocab based on /home/kevin7674/mingta/hw1/Smoking_Status_Detection/Case_Presentation/train.csv\n",
      "Vocab has 14153 entries\n",
      "Creating training data\n",
      "Creating testing data\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchtext\n",
    "\n",
    "NGRAMS = 2\n",
    "PATH = os.path.abspath(os.getcwd())\n",
    "datasetPath = PATH+\"/Case_Presentation/\"\n",
    "TraindatasetPath = datasetPath+\"train/\"\n",
    "TestdatasetPath = datasetPath+\"test/\"\n",
    "\n",
    "# if not os.path.isdir(datasetPath+'NLPdata'):\n",
    "#     os.mkdir(datasetPath+'NLPdata')\n",
    "train_dataset, test_dataset = DATASETS['Smoking_Status'](\n",
    "    root=datasetPath, ngrams=NGRAMS, vocab=None)\n",
    "BATCH_SIZE = 16\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class TextSentiment(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(train_dataset.get_vocab())\n",
    "EMBED_DIM = 32\n",
    "NUN_CLASS = len(train_dataset.get_labels())\n",
    "model = TextSentiment(VOCAB_SIZE, EMBED_DIM, NUN_CLASS).to(device)\n",
    "# print(train_dataset.__getitem__(0))\n",
    "# print(train_dataset.get_labels())\n",
    "# print(train_dataset.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(batch):\n",
    "    label = torch.tensor([entry[0] for entry in batch])\n",
    "    text = [entry[1] for entry in batch]\n",
    "    offsets = [0] + [len(entry) for entry in text]\n",
    "    # torch.Tensor.cumsum returns the cumulative sum\n",
    "    # of elements in the dimension dim.\n",
    "    # torch.Tensor([1.0, 2.0, 3.0]).cumsum(dim=0)\n",
    "\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text = torch.cat(text)\n",
    "    return text, offsets, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  51,  145, 5630,  ...,  301,  124,  130])\n",
      "tensor([    0,  1149,  1794,  3197,  4128,  4465,  5880,  6197,  6454,  7659,\n",
      "         7998,  8821, 10078, 10893, 11228, 11873])\n",
      "tensor([2, 0, 3, 1, 0, 1, 2, 0, 1, 2, 2, 2, 0, 0, 3, 1])\n",
      "tensor([ 370,  145, 4725,  ...,  301,  124,  130])\n",
      "tensor([    0,   945,  2038,  2353,  3046,  4427,  5818,  6509,  8028,  9721,\n",
      "        10890, 13225, 13772, 15425, 16248, 17061])\n",
      "tensor([3, 1, 0, 1, 2, 1, 0, 1, 2, 3, 2, 3, 3, 1, 3, 0])\n",
      "tensor([ 370,  145, 5949,  ..., 1364,  124,  130])\n",
      "tensor([   0, 1395, 2024, 2333, 3414, 4525])\n",
      "tensor([3, 1, 0, 2, 3, 0])\n",
      "Epoch: 1  | time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.1134(train)\t|\tAcc: 18.4%(train)\n",
      "\tLoss: 1.4169(valid)\t|\tAcc: 50.0%(valid)\n",
      "tensor([  51,  145, 5929,  ...,  301,  124,  130])\n",
      "tensor([    0,   317,  1428,  2233,  2568,  3259,  4072,  4409,  5224,  6481,\n",
      "         7872,  8695,  9844, 10159, 10788, 12169])\n",
      "tensor([2, 3, 0, 0, 0, 3, 0, 0, 2, 1, 1, 2, 0, 1, 2, 3])\n",
      "tensor([  370,   145,  4725,  ..., 11185, 12201, 10834])\n",
      "tensor([    0,   945,  1768,  3461,  4106,  4799,  5346,  6079,  6724,  7893,\n",
      "        10228, 10537, 11618, 13013, 14666, 15871])\n",
      "tensor([3, 2, 2, 3, 1, 3, 0, 0, 3, 2, 0, 2, 3, 3, 1, 2])\n",
      "tensor([ 355, 5598,  331,  ..., 5020,  124,  130])\n",
      "tensor([   0,  257, 1776, 2869, 4284, 5257])\n",
      "tensor([0, 1, 1, 1, 1, 1])\n",
      "Epoch: 2  | time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.1247(train)\t|\tAcc: 15.8%(train)\n",
      "\tLoss: 3.0576(valid)\t|\tAcc: 0.0%(valid)\n",
      "tensor([  355,  5993,   331,  ..., 13920,  8315, 12121])\n",
      "tensor([    0,   805,  2324,  3727,  4540,  5087,  5716,  7409,  8790,  9721,\n",
      "        11112, 11805, 12062, 12885, 13708, 16043])\n",
      "tensor([0, 1, 3, 3, 3, 1, 2, 2, 1, 1, 1, 0, 1, 2, 2, 3])\n",
      "tensor([ 355, 5737,  331,  ..., 1364,  124,  130])\n",
      "tensor([    0,   309,   624,   963,  2168,  3249,  4064,  5009,  6178,  6823,\n",
      "         7158,  8251,  8588,  9845, 10536, 11685])\n",
      "tensor([0, 0, 2, 1, 2, 0, 3, 3, 0, 0, 1, 0, 2, 0, 2, 0])\n",
      "tensor([  436,  5763,  2011,  ..., 13199,  7967, 13183])\n",
      "tensor([   0,  973, 2084, 3499, 4894, 5539])\n",
      "tensor([1, 3, 1, 3, 3, 2])\n",
      "Epoch: 3  | time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.1393(train)\t|\tAcc: 21.1%(train)\n",
      "\tLoss: 1.2320(valid)\t|\tAcc: 50.0%(valid)\n",
      "tensor([ 355, 4819,  331,  ..., 9490, 7341, 4076])\n",
      "tensor([    0,   315,   960,  1905,  2242,  2935,  4192,  5341,  6434,  7639,\n",
      "         8454,  9973, 11388, 11697, 12036, 13439])\n",
      "tensor([0, 3, 3, 0, 1, 2, 2, 1, 1, 0, 1, 1, 0, 2, 3, 3])\n",
      "tensor([  355,  5386,   331,  ...,  3298,  1650, 10635])\n",
      "tensor([    0,   335,  1148,  1881,  2704,  3527,  4172,  5563,  6254,  7335,\n",
      "         8716, 10111, 11042, 11359, 13694, 14667])\n",
      "tensor([0, 3, 0, 2, 1, 0, 1, 0, 2, 2, 3, 1, 2, 2, 1, 0])\n",
      "tensor([ 370,  145, 5771,  ..., 6784, 3012, 6080])\n",
      "tensor([   0, 1653, 2458, 3005, 4698, 5809])\n",
      "tensor([3, 0, 3, 2, 3, 1])\n",
      "Epoch: 4  | time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.1167(train)\t|\tAcc: 23.7%(train)\n",
      "\tLoss: 1.2331(valid)\t|\tAcc: 50.0%(valid)\n",
      "tensor([ 355, 4819,  331,  ..., 1364,  124,  130])\n",
      "tensor([    0,   315,  1520,  2613,  4016,  4839,  5468,  5803,  6776,  7509,\n",
      "         8766,  9457, 10872, 11953, 13606, 14551])\n",
      "tensor([0, 1, 1, 3, 2, 1, 0, 1, 0, 2, 0, 1, 2, 3, 3, 0])\n",
      "tensor([ 370,  145, 5731,  ..., 4868,  124,  130])\n",
      "tensor([    0,  1169,  3504,  4899,  6418,  7233,  8046,  9195,  9452, 10833,\n",
      "        11172, 11995, 12304, 13415, 13732, 14663])\n",
      "tensor([3, 2, 3, 1, 0, 3, 2, 0, 2, 2, 1, 0, 3, 2, 1, 2])\n",
      "tensor([ 370,  145, 5765,  ..., 4936,  124,  130])\n",
      "tensor([   0,  547, 1192, 2583, 3388, 4033])\n",
      "tensor([3, 0, 1, 0, 3, 1])\n",
      "Epoch: 5  | time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.1151(train)\t|\tAcc: 21.1%(train)\n",
      "\tLoss: 1.6082(valid)\t|\tAcc: 50.0%(valid)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_func(sub_train_):\n",
    "\n",
    "    # Train the model\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                      collate_fn=generate_batch)\n",
    "    for i, (text, offsets, cls) in enumerate(data):\n",
    "        print(text)\n",
    "        print(offsets)\n",
    "        print(cls)\n",
    "        optimizer.zero_grad()\n",
    "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "        output = model(text, offsets)\n",
    "        loss = criterion(output, cls)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    # Adjust the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    return train_loss / len(sub_train_), train_acc / len(sub_train_)\n",
    "\n",
    "def test(data_):\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
    "    for text, offsets, cls in data:\n",
    "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(text, offsets)\n",
    "            loss = criterion(output, cls)\n",
    "            loss += loss.item()\n",
    "            acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    return loss / len(data_), acc / len(data_)\n",
    "\n",
    "\n",
    "\n",
    "import time\n",
    "from torch.utils.data.dataset import random_split\n",
    "N_EPOCHS = 5\n",
    "min_valid_loss = float('inf')\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=4.0)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
    "\n",
    "train_len = int(len(train_dataset) * 0.95)\n",
    "sub_train_, sub_valid_ = \\\n",
    "    random_split(train_dataset, [train_len, len(train_dataset) - train_len])\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train_func(sub_train_)\n",
    "    valid_loss, valid_acc = test(sub_valid_)\n",
    "\n",
    "    secs = int(time.time() - start_time)\n",
    "    mins = secs / 60\n",
    "    secs = secs % 60\n",
    "\n",
    "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
    "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
    "    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
