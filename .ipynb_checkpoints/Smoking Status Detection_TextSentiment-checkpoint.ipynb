{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['UNKNOWN', 'NON-SMOKER', 'PAST SMOKER', 'CURRENT SMOKER']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "PATH = os.path.abspath(os.getcwd())\n",
    "datasetPath = PATH+\"/Case_Presentation/\"\n",
    "TraindatasetPath = datasetPath+\"train/\"\n",
    "TestdatasetPath = datasetPath+\"test/\"\n",
    "\n",
    "#Make Label file\n",
    "FileList = os.listdir(TraindatasetPath)\n",
    "ClassName = []\n",
    "\n",
    "for i in range(len(FileList)):\n",
    "    if FileList[i] != 'train.csv' and FileList[i] != 'classes.txt' and FileList[i] != '.ipynb_checkpoints':\n",
    "        tmpName = FileList[i].split(\"_\",1)[0]\n",
    "        try:\n",
    "            ClassName.index(tmpName)\n",
    "        except :\n",
    "            ClassName.append(tmpName)\n",
    "        \n",
    "print(ClassName)\n",
    "\n",
    "file = open(datasetPath+'classes.txt','w');\n",
    "for i in range(len(ClassName)):\n",
    "    file.write(str(ClassName[i])+'\\n');\n",
    "file.close();\n",
    "\n",
    "#Creat Training file\n",
    "CLASSNAME = []\n",
    "CLASSNAMES = []\n",
    "INFO = []\n",
    "FileList = os.listdir(TraindatasetPath)\n",
    "\n",
    "\n",
    "for i in range(len(FileList)):\n",
    "    filename = FileList[i]\n",
    "    tmpName = filename.split(\"_\",1)[0]\n",
    "    classN = ClassName.index(tmpName)\n",
    "    try:\n",
    "        classN = ClassName.index(tmpName)\n",
    "        CLASSNAMES.append(tmpName)\n",
    "        CLASSNAME.append(classN)\n",
    "    \n",
    "        file = open(TraindatasetPath+filename,'r')\n",
    "        info = file.read()\n",
    "        file.close()\n",
    "        INFO.append(info)\n",
    "        \n",
    "    except :\n",
    "        print(\"except {}\".format(tmpName))\n",
    "    \n",
    "\n",
    "df_train = pd.DataFrame({'label1':CLASSNAME,'label2':CLASSNAMES,'text':INFO})\n",
    "df_train.plot.hist()\n",
    "df_train.to_csv(datasetPath+\"train.csv\",index=False,sep=',',header=False)\n",
    "\n",
    "#Creat Testing file\n",
    "CLASSNAME = []\n",
    "CLASSNAMES = []\n",
    "INFO = []\n",
    "FileList = os.listdir(TestdatasetPath)\n",
    "\n",
    "\n",
    "for i in range(len(FileList)):\n",
    "    filename = FileList[i]    \n",
    "    tmpName = filename.split(\"_\",1)[0]\n",
    "    \n",
    "    CLASSNAME.append(0)\n",
    "    CLASSNAMES.append(ClassName[0])\n",
    "    \n",
    "    file = open(TestdatasetPath+filename,'r')\n",
    "    info = file.read()\n",
    "    file.close()\n",
    "    INFO.append(info)\n",
    "        \n",
    "import pandas as pd\n",
    "df_test = pd.DataFrame({'label1':CLASSNAME,'label2':CLASSNAMES,'text':INFO})\n",
    "df_test.to_csv(datasetPath+\"test.csv\",index=False,sep=',',header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import torch\n",
    "import io\n",
    "from torchtext.utils import download_from_url, extract_archive, unicode_csv_reader\n",
    "from torchtext.data.utils import ngrams_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.vocab import Vocab\n",
    "from tqdm import tqdm\n",
    "\n",
    "URLS = {\n",
    "    'Smoking_Status':\n",
    "        'https://drive.google.com/uc?export=download&id=0Bz8a_Dbh9QhbUDNpeUdjb0wxRms'\n",
    "}\n",
    "\n",
    "\n",
    "def _csv_iterator(data_path, ngrams, yield_cls=False):\n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "    with io.open(data_path, encoding=\"utf8\") as f:\n",
    "        reader = unicode_csv_reader(f)\n",
    "        for row in reader:\n",
    "            tokens = ' '.join(row[1:])\n",
    "            tokens = tokenizer(tokens)\n",
    "            if yield_cls:\n",
    "                #yield int(row[0]) - 1, ngrams_iterator(tokens, ngrams)\n",
    "                yield int(row[0]), ngrams_iterator(tokens, ngrams)\n",
    "            else:\n",
    "                yield ngrams_iterator(tokens, ngrams)\n",
    "\n",
    "\n",
    "def _create_data_from_iterator(vocab, iterator, include_unk):\n",
    "    data = []\n",
    "    labels = []\n",
    "    with tqdm(unit_scale=0, unit='lines') as t:\n",
    "        for cls, tokens in iterator:\n",
    "            if include_unk:\n",
    "                tokens = torch.tensor([vocab[token] for token in tokens])\n",
    "            else:\n",
    "                token_ids = list(filter(lambda x: x is not Vocab.UNK, [vocab[token]\n",
    "                                        for token in tokens]))\n",
    "                tokens = torch.tensor(token_ids)\n",
    "            if len(tokens) == 0:\n",
    "                logging.info('Row contains no tokens.')\n",
    "            data.append((cls, tokens))\n",
    "            labels.append(cls)\n",
    "            t.update(1)\n",
    "    return data, set(labels)\n",
    "\n",
    "\n",
    "class TextClassificationDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Defines an abstract text classification datasets.\n",
    "       Currently, we only support the following datasets:\n",
    "             - AG_NEWS\n",
    "             - SogouNews\n",
    "             - DBpedia\n",
    "             - YelpReviewPolarity\n",
    "             - YelpReviewFull\n",
    "             - YahooAnswers\n",
    "             - AmazonReviewPolarity\n",
    "             - AmazonReviewFull\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab, data, labels):\n",
    "        \"\"\"Initiate text-classification dataset.\n",
    "        Arguments:\n",
    "            vocab: Vocabulary object used for dataset.\n",
    "            data: a list of label/tokens tuple. tokens are a tensor after\n",
    "                numericalizing the string tokens. label is an integer.\n",
    "                [(label1, tokens1), (label2, tokens2), (label2, tokens3)]\n",
    "            label: a set of the labels.\n",
    "                {label1, label2}\n",
    "        Examples:\n",
    "            See the examples in examples/text_classification/\n",
    "        \"\"\"\n",
    "\n",
    "        super(TextClassificationDataset, self).__init__()\n",
    "        self._data = data\n",
    "        self._labels = labels\n",
    "        self._vocab = vocab\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self._data[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for x in self._data:\n",
    "            yield x\n",
    "\n",
    "    def get_labels(self):\n",
    "        return self._labels\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return self._vocab\n",
    "\n",
    "\n",
    "def _setup_datasets(dataset_name, root='.data', ngrams=1, vocab=None, include_unk=False):\n",
    "    #dataset_tar = download_from_url(URLS[dataset_name], root=root)\n",
    "    #extracted_files = extract_archive(dataset_tar)\n",
    "    rootPath =root\n",
    "    extracted_files = [rootPath+'train.csv',rootPath+'test.csv',rootPath+'classes.txt',rootPath+'readme.txt']   \n",
    "    for fname in extracted_files:\n",
    "        if fname.endswith('train.csv'):\n",
    "            train_csv_path = fname\n",
    "        #if fname.endswith('test.csv'):\n",
    "        if fname.endswith('train.csv'):    \n",
    "            test_csv_path = fname\n",
    "\n",
    "    if vocab is None:\n",
    "        print('Building Vocab based on {}'.format(train_csv_path))\n",
    "        vocab = build_vocab_from_iterator(_csv_iterator(train_csv_path, ngrams))\n",
    "    else:\n",
    "        if not isinstance(vocab, Vocab):\n",
    "            raise TypeError(\"Passed vocabulary is not of type Vocab\")\n",
    "    print('Vocab has {} entries'.format(len(vocab)))\n",
    "    \n",
    "    print('Creating training data')\n",
    "    train_data, train_labels = _create_data_from_iterator(\n",
    "        vocab, _csv_iterator(train_csv_path, ngrams, yield_cls=True), include_unk)\n",
    "    \n",
    "    print('Creating testing data')\n",
    "    test_data, test_labels = _create_data_from_iterator(\n",
    "        vocab, _csv_iterator(test_csv_path, ngrams, yield_cls=True), include_unk)\n",
    "    \n",
    "    \n",
    "    print(train_labels)\n",
    "    print(test_labels)\n",
    "    \n",
    "   \n",
    "    if len(train_labels ^ test_labels) > 0:\n",
    "        raise ValueError(\"Training and test labels don't match\")\n",
    "    return (TextClassificationDataset(vocab, train_data, train_labels),\n",
    "            TextClassificationDataset(vocab, test_data, test_labels))\n",
    "\n",
    "\n",
    "def Smoking_Status(*args, **kwargs):\n",
    "    \"\"\" Defines Smoking_Status datasets.\n",
    "        The labels includes:\n",
    "            - 0 : UNKNOWN\n",
    "            - 1 : NON-SMOKER\n",
    "            - 2 : PAST SMOKER\n",
    "            - 3 : CURRENT SMOKER\n",
    "    Create supervised learning dataset: Smoking_Status\n",
    "    Separately returns the training and test dataset\n",
    "    Arguments:\n",
    "        root: Directory where the datasets are saved. Default: \".data\"\n",
    "        ngrams: a contiguous sequence of n items from s string text.\n",
    "            Default: 1\n",
    "        vocab: Vocabulary used for dataset. If None, it will generate a new\n",
    "            vocabulary based on the train data set.\n",
    "        include_unk: include unknown token in the data (Default: False)\n",
    "    Examples:\n",
    "        >>> train_dataset, test_dataset = torchtext.datasets.Smoking_Status(ngrams=3)\n",
    "    \"\"\"\n",
    "\n",
    "    return _setup_datasets(*((\"Smoking_Status\",) + args), **kwargs)\n",
    "\n",
    "\n",
    "DATASETS = {\n",
    "    'Smoking_Status': Smoking_Status\n",
    "}\n",
    "\n",
    "\n",
    "LABELS = {\n",
    "    'Smoking_Status': {0: 'UNKNOWN',\n",
    "                1: 'NON-SMOKER',\n",
    "                2: 'PAST SMOKER',\n",
    "                3: 'CURRENT SMOKER'}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40lines [00:00, 2326.65lines/s]\n",
      "40lines [00:00, 1073.28lines/s]\n",
      "40lines [00:00, 1067.36lines/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Vocab based on /home/kevin7674/mingta/minGPT/Case_Presentation/train.csv\n",
      "Vocab has 14153 entries\n",
      "Creating training data\n",
      "Creating testing data\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchtext\n",
    "\n",
    "NGRAMS = 2\n",
    "PATH = os.path.abspath(os.getcwd())\n",
    "datasetPath = PATH+\"/Case_Presentation/\"\n",
    "TraindatasetPath = datasetPath+\"train/\"\n",
    "TestdatasetPath = datasetPath+\"test/\"\n",
    "\n",
    "# if not os.path.isdir(datasetPath+'NLPdata'):\n",
    "#     os.mkdir(datasetPath+'NLPdata')\n",
    "train_dataset, test_dataset = DATASETS['Smoking_Status'](\n",
    "    root=datasetPath, ngrams=NGRAMS, vocab=None)\n",
    "BATCH_SIZE = 16\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(train_dataset.get_vocab())\n",
    "EMBED_DIM = 32\n",
    "NUN_CLASS = len(train_dataset.get_labels())\n",
    "model = TextSentiment(VOCAB_SIZE, EMBED_DIM, NUN_CLASS).to(device)\n",
    "# print(train_dataset.__getitem__(0))\n",
    "# print(train_dataset.get_labels())\n",
    "# print(train_dataset.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(batch):\n",
    "    label = torch.tensor([entry[0] for entry in batch])\n",
    "    text = [entry[1] for entry in batch]\n",
    "    offsets = [0] + [len(entry) for entry in text]\n",
    "    # torch.Tensor.cumsum returns the cumulative sum\n",
    "    # of elements in the dimension dim.\n",
    "    # torch.Tensor([1.0, 2.0, 3.0]).cumsum(dim=0)\n",
    "\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text = torch.cat(text)\n",
    "    return text, offsets, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 355, 5598,  331,  ...,  301,  124,  130])\n",
      "tensor([    0,   257,  2592,  3565,  4210,  5155,  5888,  7541,  8088,  9293,\n",
      "         9610, 10779, 11602, 12417, 13348, 14459])\n",
      "tensor([0, 2, 1, 3, 3, 0, 3, 3, 1, 2, 3, 2, 0, 1, 3, 0])\n",
      "tensor([ 355, 5795,  418,  ..., 1364,  124,  130])\n",
      "tensor([    0,   691,  1514,  2607,  3420,  3735,  5116,  5451,  5790,  7185,\n",
      "         8704,  9853, 10992, 12249, 12558, 13359])\n",
      "tensor([0, 1, 1, 3, 0, 2, 0, 2, 3, 1, 2, 3, 2, 0, 2, 0])\n",
      "tensor([ 355, 5759,  331,  ..., 4960,  124,  130])\n",
      "tensor([   0,  645, 2060, 3463, 5156, 5785])\n",
      "tensor([0, 1, 3, 2, 1, 2])\n",
      "Epoch: 1  | time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0970(train)\t|\tAcc: 39.5%(train)\n",
      "\tLoss: 0.8666(valid)\t|\tAcc: 100.0%(valid)\n",
      "tensor([  51,  145, 5541,  ...,  301,  124,  130])\n",
      "tensor([    0,   823,  1934,  2907,  4426,  4741,  5078,  5387,  6782,  7921,\n",
      "         9090, 10171, 10984, 12077, 14412, 15103])\n",
      "tensor([2, 3, 1, 1, 0, 0, 0, 3, 3, 3, 2, 3, 1, 2, 0, 2])\n",
      "tensor([  51,  145, 5630,  ..., 5020,  124,  130])\n",
      "tensor([    0,  1149,  1488,  2903,  4596,  5143,  5460,  6261,  7076,  7411,\n",
      "         8056,  8685,  9630, 10453, 11710, 12915])\n",
      "tensor([2, 2, 1, 2, 3, 2, 2, 0, 0, 3, 1, 3, 1, 2, 1, 1])\n",
      "tensor([  355,  5610,   331,  ...,  3298,  1650, 10635])\n",
      "tensor([   0,  733, 1538, 2183, 3836, 5239])\n",
      "tensor([0, 0, 0, 3, 3, 0])\n",
      "Epoch: 2  | time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0948(train)\t|\tAcc: 47.4%(train)\n",
      "\tLoss: 1.6829(valid)\t|\tAcc: 0.0%(valid)\n",
      "tensor([ 436, 5173, 1401,  ...,  301,  124,  130])\n",
      "tensor([    0,  1093,  2350,  4003,  4550,  5481,  6426,  6761,  7406,  8575,\n",
      "         9220, 10033, 10372, 11345, 11682, 12831])\n",
      "tensor([1, 2, 3, 3, 1, 3, 0, 0, 3, 3, 3, 2, 1, 0, 2, 0])\n",
      "tensor([ 370,  145, 4963,  ..., 1470,  124,  130])\n",
      "tensor([    0,  1139,  2220,  3043,  4154,  4463,  5286,  6087,  6402,  7783,\n",
      "         9302, 10117, 10434, 11125, 12540, 13745])\n",
      "tensor([3, 2, 2, 3, 0, 1, 2, 0, 2, 1, 0, 2, 0, 1, 1, 3])\n",
      "tensor([ 355, 5598,  331,  ..., 1364,  124,  130])\n",
      "tensor([   0,  257, 2592, 3995, 4624, 6317])\n",
      "tensor([0, 2, 3, 1, 2, 0])\n",
      "Epoch: 3  | time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0898(train)\t|\tAcc: 39.5%(train)\n",
      "\tLoss: 1.3124(valid)\t|\tAcc: 0.0%(valid)\n",
      "tensor([ 370,  145, 4725,  ...,  301,  124,  130])\n",
      "tensor([    0,   945,  1876,  3529,  4174,  6509,  7766,  8105,  9244, 10337,\n",
      "        11856, 12589, 13234, 13571, 13888, 14711])\n",
      "tensor([3, 1, 3, 3, 2, 2, 2, 3, 1, 1, 0, 0, 0, 2, 2, 2])\n",
      "tensor([  51,  145, 5330,  ..., 9490, 7341, 4076])\n",
      "tensor([    0,  1081,  2192,  2449,  3264,  4065,  5038,  5843,  7238,  8061,\n",
      "         9266,  9581, 10210, 11359, 11668, 13361])\n",
      "tensor([2, 3, 0, 0, 2, 1, 0, 3, 1, 1, 0, 1, 2, 0, 2, 3])\n",
      "tensor([ 370,  145, 5402,  ..., 1470,  124,  130])\n",
      "tensor([   0, 1403, 2216, 3631, 4178, 4513])\n",
      "tensor([3, 3, 1, 3, 0, 0])\n",
      "Epoch: 4  | time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0869(train)\t|\tAcc: 50.0%(train)\n",
      "\tLoss: 1.2783(valid)\t|\tAcc: 0.0%(valid)\n",
      "tensor([   51,   145,  5909,  ..., 10900, 11186, 13661])\n",
      "tensor([    0,  1381,  1690,  2495,  2810,  3783,  4988,  6245,  7190,  8283,\n",
      "         8600,  9229,  9874, 11289, 12220, 12477])\n",
      "tensor([2, 0, 0, 0, 1, 1, 2, 3, 1, 2, 1, 0, 1, 1, 0, 3])\n",
      "tensor([ 436, 5775,  331,  ...,  301,  124,  130])\n",
      "tensor([    0,   823,  1904,  3043,  3734,  4071,  4886,  6405,  6740,  7287,\n",
      "         9622,  9961, 11110, 12803, 13972, 15625])\n",
      "tensor([1, 2, 3, 0, 0, 0, 1, 0, 3, 2, 2, 2, 2, 3, 3, 3])\n",
      "tensor([  370,   145,  5838,  ...,  2858, 12360,  8313])\n",
      "tensor([   0,  645, 1378, 2201, 3596, 4397])\n",
      "tensor([3, 0, 2, 3, 2, 3])\n",
      "Epoch: 5  | time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0924(train)\t|\tAcc: 39.5%(train)\n",
      "\tLoss: 1.8249(valid)\t|\tAcc: 0.0%(valid)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_func(sub_train_):\n",
    "\n",
    "    # Train the model\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                      collate_fn=generate_batch)\n",
    "    for i, (text, offsets, cls) in enumerate(data):\n",
    "        print(text)\n",
    "        print(offsets)\n",
    "        print(cls)\n",
    "        optimizer.zero_grad()\n",
    "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "        output = model(text, offsets)\n",
    "        loss = criterion(output, cls)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    # Adjust the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    return train_loss / len(sub_train_), train_acc / len(sub_train_)\n",
    "\n",
    "def test(data_):\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
    "    for text, offsets, cls in data:\n",
    "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(text, offsets)\n",
    "            loss = criterion(output, cls)\n",
    "            loss += loss.item()\n",
    "            acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    return loss / len(data_), acc / len(data_)\n",
    "\n",
    "\n",
    "\n",
    "import time\n",
    "from torch.utils.data.dataset import random_split\n",
    "N_EPOCHS = 5\n",
    "min_valid_loss = float('inf')\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=4.0)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
    "\n",
    "train_len = int(len(train_dataset) * 0.95)\n",
    "sub_train_, sub_valid_ = \\\n",
    "    random_split(train_dataset, [train_len, len(train_dataset) - train_len])\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train_func(sub_train_)\n",
    "    valid_loss, valid_acc = test(sub_valid_)\n",
    "\n",
    "    secs = int(time.time() - start_time)\n",
    "    mins = secs / 60\n",
    "    secs = secs % 60\n",
    "\n",
    "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
    "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
    "    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
